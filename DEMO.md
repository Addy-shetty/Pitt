
# PITT Live Demonstration: Testing a Vulnerable LLM Application

This document provides a step-by-step walkthrough of using the Prompt Injection Testing Tool (PITT) to identify vulnerabilities in a live, local LLM application powered by Ollama and Gemma-2b.

---

## 1. The Target Environment

To conduct a realistic test, a simple but vulnerable Flask application (`vulnerable_app.py`) was created. 

*   **Framework:** Flask
*   **LLM Backend:** A local Ollama server.
*   **Model:** `gemma:2b`
*   **Endpoint:** The application exposes a `/chat` endpoint at `http://localhost:8080` that accepts a JSON payload with a `prompt` field.

## 2. PITT Configuration

PITT was configured via `config.yaml` to target the local Flask application. No API key was required, and the Judge LLM was disabled for this initial test to focus on direct keyword-based detection.

```yaml
target_url: "http://localhost:8080/chat"
api_key: "" 
request_method: "POST"
request_body_template:
  prompt: "{payload}"

judge_llm:
  enabled: false
```

## 3. The Test Execution

The test was performed using the PITT command-line interface (CLI).

1.  The vulnerable Flask application was started in one terminal.
2.  PITT was executed in a second terminal with the default settings, running all available rules with a single iteration:
    ```bash
    python pitt.py
    ```

## 4. Results & Analysis

PITT successfully identified several vulnerabilities by sending a series of payloads and analyzing the responses from the `gemma:2b` model.

### CLI Output

The following table was generated by PITT upon completion of the test run. It clearly indicates which attacks were successful.

```text
PASTE THE RESULTS TABLE YOU COPIED FROM YOUR TERMINAL HERE
```

### Analysis of Key Findings

*   **Direct Injection (`direct_injection_pwned`, `direct_injection_pirate`):** The tests successfully demonstrated that the model's instructions could be completely ignored in favor of the malicious prompt. The model responded with "Pwned!" and "Ahoy, matey!", proving a fundamental lack of instruction hierarchy.
*   **Role-Playing (`role_playing_pitt`):** The model was easily coerced into adopting a new persona, which is a common primer for more advanced attacks.
*   **Vulnerability Chaining (`vulnerability_chaining_ssrf`, `vulnerability_chaining_sqli`):** While the local app has no real database or internal services, the model's responses likely indicated a *willingness* to execute the malicious requests (e.g., by repeating the SQL query or acknowledging the request to access a local service). This shows a failure to recognize malicious patterns in user input.

## 5. The HTML Report

In addition to the CLI output, PITT generated a `report.html` file, providing a shareable, human-readable summary of these findings, suitable for distribution to a wider team.

## Conclusion

This demonstration proves the effectiveness of PITT as a reliable and easy-to-use tool for automatically identifying critical prompt injection vulnerabilities in LLM-integrated applications. The test took only a few minutes to set up and run, yet it provided clear, actionable evidence of multiple security flaws.
